name: ðŸŒŒ Ultimate Quantum-AI Omni-Proxy Evolution System v3.1 [Top 3 Elite Edition]

on:
  workflow_dispatch:
    inputs:
      scan_mode:
        description: 'Scanning Mode Selection'
        default: 'hybrid_quantum_ultra'
        type: choice
        options:
          - 'quantum_speed'
          - 'ultra_precise'
          - 'hybrid_quantum_ultra'
      target_count:
        description: 'Maximum proxies to scan'
        default: '2000'
        type: string
      min_score_threshold:
        description: 'Minimum acceptable score'
        default: '500'
        type: string
      force_db_cleanup:
        description: 'Force aggressive database cleanup'
        default: true
        type: boolean
  schedule:
    - cron: '0 */2 * * *'

concurrency:
  group: quantum-omni-ultimate-v3
  cancel-in-progress: true

permissions:
  contents: write
  actions: write

env:
  CARGO_TERM_COLOR: always
  RUST_BACKTRACE: 1
  WEIGHT_LATENCY: "0.35"
  WEIGHT_STABILITY: "0.25" 
  WEIGHT_TTFB: "0.30"
  WEIGHT_JITTER: "0.10"
  MAX_LATENCY_MS: "300"
  MAX_TTFB_MS: "500"
  OPTIMAL_LATENCY_MS: "80"
  OPTIMAL_TTFB_MS: "150"
  TOP_PROXIES_TO_KEEP: "3"

jobs:
  quantum-neural-ultimate-evolution:
    name: ðŸ§  Ultimate AI-Driven Proxy Evolution Engine v3.1
    runs-on: ubuntu-latest
    timeout-minutes: 180

    steps:
      - name: ðŸ“¥ Initialize Quantum Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: ðŸ” Environment Validation & Setup
        run: |
          echo "ðŸŒŸ =========================================="
          echo "ðŸŒŸ QUANTUM-AI PROXY EVOLUTION SYSTEM v3.1"
          echo "ðŸŒŸ [TOP 3 ELITE EDITION - SMART CLEANUP]"
          echo "ðŸŒŸ =========================================="
          echo ""
          echo "ðŸ“Š Configuration Matrix:"
          echo "  â€¢ Scan Mode: ${{ github.event.inputs.scan_mode || 'hybrid_quantum_ultra' }}"
          echo "  â€¢ Target Count: ${{ github.event.inputs.target_count || '2000' }}"
          echo "  â€¢ Min Score: ${{ github.event.inputs.min_score_threshold || '500' }}"
          echo "  â€¢ Keep Top: $TOP_PROXIES_TO_KEEP proxies only"
          echo "  â€¢ Force Cleanup: ${{ github.event.inputs.force_db_cleanup || 'true' }}"
          echo "  â€¢ Latency Weight: $WEIGHT_LATENCY"
          echo "  â€¢ TTFB Weight: $WEIGHT_TTFB"
          echo "  â€¢ Stability Weight: $WEIGHT_STABILITY"
          echo "  â€¢ Jitter Weight: $WEIGHT_JITTER"
          echo ""
          
          if [ -z "${{ secrets.CLOUDFLARE_API_TOKEN }}" ]; then
            echo "âš ï¸  Warning: Cloudflare API token not configured"
            echo "â„¹ï¸  D1 database features will be disabled"
          else
            echo "âœ… D1 Database integration enabled"
          fi
          
          mkdir -p data results logs cache backups
          
          echo ""
          echo "ðŸ–¥ï¸  System Resources:"
          echo "  â€¢ CPU Cores: $(nproc)"
          echo "  â€¢ Memory: $(free -h | awk '/^Mem:/ {print $2}')"
          echo "  â€¢ Disk: $(df -h / | awk 'NR==2 {print $4}')"
          echo "  â€¢ Workflow Run: #${{ github.run_number }}"
          echo ""

      - name: ðŸš€ Activate Ultimate Kernel Optimization
        run: |
          echo "ðŸ”§ Injecting Advanced Kernel Optimizations..."
          
          sudo sysctl -w net.core.default_qdisc=fq 2>/dev/null || true
          sudo sysctl -w net.ipv4.tcp_congestion_control=bbr 2>/dev/null || true
          sudo sysctl -w net.ipv4.tcp_tw_reuse=1 2>/dev/null || true
          sudo sysctl -w net.ipv4.tcp_fin_timeout=15 2>/dev/null || true
          sudo sysctl -w net.ipv4.ip_local_port_range="1024 65535" 2>/dev/null || true
          sudo sysctl -w net.core.rmem_max=134217728 2>/dev/null || true
          sudo sysctl -w net.core.wmem_max=134217728 2>/dev/null || true
          sudo sysctl -w net.core.rmem_default=16777216 2>/dev/null || true
          sudo sysctl -w net.core.wmem_default=16777216 2>/dev/null || true
          sudo sysctl -w net.ipv4.tcp_rmem="4096 87380 134217728" 2>/dev/null || true
          sudo sysctl -w net.ipv4.tcp_wmem="4096 65536 134217728" 2>/dev/null || true
          sudo sysctl -w net.ipv4.tcp_max_syn_backlog=8192 2>/dev/null || true
          sudo sysctl -w net.core.somaxconn=8192 2>/dev/null || true
          sudo sysctl -w net.core.netdev_max_backlog=16384 2>/dev/null || true
          sudo sysctl -w net.ipv4.tcp_slow_start_after_idle=0 2>/dev/null || true
          sudo sysctl -w net.ipv4.tcp_fastopen=3 2>/dev/null || true
          sudo sysctl -w net.ipv4.tcp_mtu_probing=1 2>/dev/null || true
          sudo sysctl -w net.ipv4.tcp_keepalive_time=600 2>/dev/null || true
          sudo sysctl -w net.ipv4.tcp_keepalive_probes=3 2>/dev/null || true
          sudo sysctl -w net.ipv4.tcp_keepalive_intvl=15 2>/dev/null || true
          sudo sysctl -w vm.swappiness=10 2>/dev/null || true
          sudo sysctl -w fs.file-max=2097152 2>/dev/null || true
          
          ulimit -n 1048576 2>/dev/null || echo "â„¹ï¸  ulimit modification not permitted (normal in GitHub Actions)"
          
          BBR_STATUS=$(sysctl net.ipv4.tcp_congestion_control 2>/dev/null | cut -d'=' -f2 | xargs || echo "unknown")
          if [ "$BBR_STATUS" == "bbr" ]; then
            echo "âœ… Google BBR Activated Successfully"
          else
            echo "â„¹ï¸  BBR status: $BBR_STATUS (using system default)"
          fi
          
          echo "âœ… Kernel operating at maximum available performance"

      - name: ðŸ› ï¸ Install Advanced Computational Tools
        run: |
          echo "ðŸ“¦ Installing high-performance toolchain..."
          
          sudo apt-get update -qq
          sudo apt-get install -y --no-install-recommends \
            build-essential \
            jq \
            curl \
            wget \
            netcat-openbsd \
            iputils-ping \
            traceroute \
            dnsutils \
            parallel \
            bc \
            python3-pip \
            geoip-bin \
            geoip-database \
            geoip-database-extra \
            libssl-dev \
            libcurl4-openssl-dev \
            pkg-config \
            mtr-tiny \
            nmap \
            httping 2>/dev/null || true
          
          pip3 install --quiet numpy scipy 2>/dev/null || echo "â„¹ï¸  Python analytics libraries are optional"
          
          echo "âœ… Advanced toolchain loaded and ready"

      - name: ðŸ§¹ Elite Database Cleanup - Keep Top 3 Only
        id: db_cleanup
        continue-on-error: true
        env:
          CF_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CF_ACCOUNT_ID: ${{ secrets.CF_ACCOUNT_ID }}
          D1_DATABASE_ID: ${{ secrets.D1_DATABASE_ID }}
          FORCE_CLEANUP: ${{ github.event.inputs.force_db_cleanup }}
        run: |
          echo "ðŸ§¹ Initiating Elite Top 3 Database Cleanup System..."
          
          if [ -z "$CF_API_TOKEN" ]; then
            echo "âš ï¸  D1 not configured, skipping cleanup"
            echo "cleanup_performed=false" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          TIMESTAMP=$(date +%s)
          
          echo "ðŸ’¾ Creating safety backup of current state..."
          SQL_BACKUP="SELECT * FROM proxy_health ORDER BY (total_score * success_rate) DESC LIMIT 10;"
          
          BACKUP_RESP=$(curl -s --max-time 10 -X POST \
            "https://api.cloudflare.com/client/v4/accounts/$CF_ACCOUNT_ID/d1/database/$D1_DATABASE_ID/query" \
            -H "Authorization: Bearer $CF_API_TOKEN" \
            -H "Content-Type: application/json" \
            --data "$(jq -n --arg sql "$SQL_BACKUP" '{sql: $sql}')" 2>/dev/null)
          
          if echo "$BACKUP_RESP" | grep -q '"success":true' 2>/dev/null; then
            echo "$BACKUP_RESP" | jq '.' > backups/state_backup_$TIMESTAMP.json 2>/dev/null || true
            echo "âœ… Safety backup completed"
          fi
          
          SQL_STATS="SELECT COUNT(*) as total, 
                            SUM(CASE WHEN is_healthy = 1 THEN 1 ELSE 0 END) as healthy,
                            SUM(CASE WHEN is_healthy = 0 THEN 1 ELSE 0 END) as unhealthy,
                            SUM(CASE WHEN is_current_best = 1 THEN 1 ELSE 0 END) as champions
                     FROM proxy_health;"
          
          STATS_RESP=$(curl -s --max-time 10 -X POST \
            "https://api.cloudflare.com/client/v4/accounts/$CF_ACCOUNT_ID/d1/database/$D1_DATABASE_ID/query" \
            -H "Authorization: Bearer $CF_API_TOKEN" \
            -H "Content-Type: application/json" \
            --data "$(jq -n --arg sql "$SQL_STATS" '{sql: $sql}')" 2>/dev/null)
          
          TOTAL_BEFORE=$(echo "$STATS_RESP" | jq -r '.result[0].results[0].total // 0' 2>/dev/null)
          HEALTHY_BEFORE=$(echo "$STATS_RESP" | jq -r '.result[0].results[0].healthy // 0' 2>/dev/null)
          UNHEALTHY_BEFORE=$(echo "$STATS_RESP" | jq -r '.result[0].results[0].unhealthy // 0' 2>/dev/null)
          
          echo "ðŸ“Š Database Statistics (Before Cleanup):"
          echo "  â€¢ Total Proxies: $TOTAL_BEFORE"
          echo "  â€¢ Healthy: $HEALTHY_BEFORE"
          echo "  â€¢ Unhealthy: $UNHEALTHY_BEFORE"
          echo ""
          
          echo "ðŸ—‘ï¸  Phase 1: AGGRESSIVE CLEANUP - Keeping ONLY Top 3 Elite Performers..."
          
          SQL_CLEANUP_ELITE="DELETE FROM proxy_health 
                             WHERE ip_port NOT IN (
                               SELECT ip_port FROM (
                                 SELECT ip_port FROM proxy_health 
                                 ORDER BY (total_score * success_rate * quality_index) DESC 
                                 LIMIT $TOP_PROXIES_TO_KEEP
                               )
                             );"
          
          CLEANUP_RESP=$(curl -s --max-time 20 -X POST \
            "https://api.cloudflare.com/client/v4/accounts/$CF_ACCOUNT_ID/d1/database/$D1_DATABASE_ID/query" \
            -H "Authorization: Bearer $CF_API_TOKEN" \
            -H "Content-Type: application/json" \
            --data "$(jq -n --arg sql "$SQL_CLEANUP_ELITE" '{sql: $sql}')" 2>/dev/null)
          
          if echo "$CLEANUP_RESP" | grep -q '"success":true' 2>/dev/null; then
            echo "âœ… Elite cleanup completed - Only Top $TOP_PROXIES_TO_KEEP remain"
          else
            echo "âš ï¸  Cleanup response: $(echo "$CLEANUP_RESP" | jq -r '.errors[0].message // "Unknown"' 2>/dev/null)"
          fi
          
          echo "ðŸ”§ Phase 2: Champion flag consistency check..."
          
          SQL_RESET_ALL="UPDATE proxy_health SET is_current_best = 0;"
          curl -s --max-time 10 -X POST \
            "https://api.cloudflare.com/client/v4/accounts/$CF_ACCOUNT_ID/d1/database/$D1_DATABASE_ID/query" \
            -H "Authorization: Bearer $CF_API_TOKEN" \
            -H "Content-Type: application/json" \
            --data "$(jq -n --arg sql "$SQL_RESET_ALL" '{sql: $sql}')" > /dev/null 2>&1
          
          echo "âœ… Champion flags reset"
          
          FINAL_STATS=$(curl -s --max-time 10 -X POST \
            "https://api.cloudflare.com/client/v4/accounts/$CF_ACCOUNT_ID/d1/database/$D1_DATABASE_ID/query" \
            -H "Authorization: Bearer $CF_API_TOKEN" \
            -H "Content-Type: application/json" \
            --data "$(jq -n --arg sql "$SQL_STATS" '{sql: $sql}')" 2>/dev/null)
          
          TOTAL_AFTER=$(echo "$FINAL_STATS" | jq -r '.result[0].results[0].total // 0' 2>/dev/null)
          REMOVED=$((TOTAL_BEFORE - TOTAL_AFTER))
          
          echo ""
          echo "ðŸ“Š Elite Cleanup Summary:"
          echo "  â€¢ Proxies Removed: $REMOVED"
          echo "  â€¢ Elite Proxies Remaining: $TOTAL_AFTER (Maximum: $TOP_PROXIES_TO_KEEP)"
          echo "  â€¢ Database Optimized: âœ…"
          echo ""
          
          echo "cleanup_performed=true" >> $GITHUB_OUTPUT
          echo "removed_count=$REMOVED" >> $GITHUB_OUTPUT
          echo "remaining_count=$TOTAL_AFTER" >> $GITHUB_OUTPUT

      - name: ðŸ§  AI Deep Memory Recall from D1 Database
        id: memory_recall
        continue-on-error: true
        env:
          CF_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CF_ACCOUNT_ID: ${{ secrets.CF_ACCOUNT_ID }}
          D1_DATABASE_ID: ${{ secrets.D1_DATABASE_ID }}
        run: |
          echo "ðŸ” Accessing Long-Term AI Memory (Cloudflare D1)..."
          
          if [ -z "$CF_API_TOKEN" ]; then
            echo "âš ï¸  D1 not configured, using fresh scan only"
            touch data/memory_proxies.txt
            echo "memory_count=0" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          CUTOFF_TIME=$(date -d '48 hours ago' +%s 2>/dev/null || echo "0")
          
          SQL_QUERY="SELECT ip_port, total_score, latency_ms, ttfb_ms, location, success_rate, check_count 
                     FROM proxy_health 
                     WHERE is_healthy=1 
                     AND last_check > $CUTOFF_TIME 
                     AND success_rate > 60
                     ORDER BY (total_score * success_rate * CASE WHEN check_count > 5 THEN 1.2 ELSE 1.0 END) DESC 
                     LIMIT 150;"
          
          RESPONSE=$(curl -s --max-time 15 -X POST \
            "https://api.cloudflare.com/client/v4/accounts/$CF_ACCOUNT_ID/d1/database/$D1_DATABASE_ID/query" \
            -H "Authorization: Bearer $CF_API_TOKEN" \
            -H "Content-Type: application/json" \
            --data "$(jq -n --arg sql "$SQL_QUERY" '{sql: $sql}')" 2>/dev/null)
          
          if echo "$RESPONSE" | grep -q '"success":true' 2>/dev/null; then
            echo "$RESPONSE" | jq -r '.result[0].results[]? | .ip_port' > data/memory_proxies.txt 2>/dev/null || touch data/memory_proxies.txt
            MEM_COUNT=$(wc -l < data/memory_proxies.txt 2>/dev/null || echo "0")
            echo "âœ… AI Successfully Recalled $MEM_COUNT high-performance nodes from history"
            echo "memory_count=$MEM_COUNT" >> $GITHUB_OUTPUT
            
            echo "$RESPONSE" | jq -r '.result[0].results[]? | "\(.ip_port)|\(.total_score)|\(.success_rate)"' > cache/historical_scores.txt 2>/dev/null || true
            
            if [ "$MEM_COUNT" -gt 0 ]; then
              echo ""
              echo "ðŸ† Top 5 Recalled Champions:"
              head -5 data/memory_proxies.txt | nl
            fi
          else
            echo "âš ï¸  D1 query returned no results, proceeding with fresh scan"
            touch data/memory_proxies.txt
            echo "memory_count=0" >> $GITHUB_OUTPUT
          fi

      - name: ðŸŒ Advanced Global Proxy Harvesting
        run: |
          echo "ðŸ“¡ Initiating global proxy spectrum scan..."
          
          declare -a SOURCES=(
            "https://raw.githubusercontent.com/TheSpeedX/SOCKS-List/master/http.txt"
            "https://raw.githubusercontent.com/monosans/proxy-list/main/proxies/http.txt"
            "https://raw.githubusercontent.com/proxifly/free-proxy-list/main/proxies/protocols/http/data.txt"
            "https://raw.githubusercontent.com/zloi-user/hideip.me/main/http.txt"
            "https://raw.githubusercontent.com/prxchk/proxy-list/main/http.txt"
            "https://raw.githubusercontent.com/clarketm/proxy-list/master/proxy-list-raw.txt"
            "https://raw.githubusercontent.com/ShiftyTR/Proxy-List/master/http.txt"
            "https://raw.githubusercontent.com/jetkai/proxy-list/main/online-proxies/txt/proxies-http.txt"
            "https://api.proxyscrape.com/v2/?request=get&protocol=http&timeout=10000&country=all&simplified=true"
            "https://www.proxy-list.download/api/v1/get?type=http"
            "https://raw.githubusercontent.com/hookzof/socks5_list/master/proxy.txt"
            "https://raw.githubusercontent.com/mertguvencli/http-proxy-list/main/proxy-list/data.txt"
            "https://raw.githubusercontent.com/almroot/proxylist/master/list.txt"
          )
          
          echo "ðŸ”„ Fetching from ${#SOURCES[@]} global sources..."
          
          touch data/fresh_proxies.txt
          
          fetch_source() {
            local url="$1"
            local output="$2"
            curl -sL --max-time 8 --retry 2 --retry-delay 1 "$url" >> "$output" 2>/dev/null || true
          }
          export -f fetch_source
          
          if command -v parallel &> /dev/null; then
            printf "%s\n" "${SOURCES[@]}" | parallel -j 10 "fetch_source {} data/fresh_proxies.txt"
          else
            for src in "${SOURCES[@]}"; do
              fetch_source "$src" "data/fresh_proxies.txt"
            done
          fi
          
          FRESH_COUNT=$(wc -l < data/fresh_proxies.txt 2>/dev/null || echo "0")
          echo "âœ… Raw data collected: $FRESH_COUNT entries"
          
          cat data/memory_proxies.txt data/fresh_proxies.txt > data/raw_combined.txt 2>/dev/null || touch data/raw_combined.txt
          
          grep -Eo "([0-9]{1,3}\.){3}[0-9]{1,3}:[0-9]{1,5}" data/raw_combined.txt 2>/dev/null | \
            awk -F':' '$2 > 0 && $2 < 65536' | \
            sort -u | \
            shuf | \
            head -n ${{ github.event.inputs.target_count || '2000' }} > data/targets.txt
          
          sed -i '/^0\.0\.0\.0/d; /^127\./d; /^255\./d; /^169\.254\./d; /^224\./d; /^10\./d; /^172\.16\./d; /^192\.168\./d' data/targets.txt 2>/dev/null || true
          
          TARGET_COUNT=$(wc -l < data/targets.txt 2>/dev/null || echo "0")
          echo "âœ… Target acquisition complete: $TARGET_COUNT unique candidates"
          
          if [ "$TARGET_COUNT" -lt 10 ]; then
            echo "âš ï¸  Low target count, injecting failsafe nodes"
            cat >> data/targets.txt << 'FAILSAFE'
          8.8.8.8:80
          1.1.1.1:80
          FAILSAFE
          fi
          
          echo ""
          echo "ðŸ“Š Harvesting Statistics:"
          echo "  â€¢ Raw entries collected: $(wc -l < data/raw_combined.txt 2>/dev/null || echo '0')"
          echo "  â€¢ Final targets: $(wc -l < data/targets.txt 2>/dev/null || echo '0')"
          echo "  â€¢ Memory proxies included: $(wc -l < data/memory_proxies.txt 2>/dev/null || echo '0')"

      - name: ðŸ¦€ Setup Advanced Rust Toolchain
        uses: dtolnay/rust-toolchain@stable
        with:
          toolchain: stable

      - name: âš¡ Build Ultimate Quantum Scanner Engine v3.1
        run: |
          echo "âš™ï¸  Generating advanced Rust scanner with enhanced multi-dimensional analysis..."
          
          cat > main.rs << 'RUST_ENGINE_EOF'
          use std::env;
          use std::fs::File;
          use std::io::{BufRead, BufReader, Read, Write};
          use std::net::{TcpStream, SocketAddr};
          use std::time::{Duration, Instant};
          use std::thread;
          use std::sync::{Arc, Mutex};

          #[derive(Clone, Debug)]
          struct ProxyNode {
              ip_port: String,
              tcp_latency: u128,
              ttfb: u128,
              jitter: u128,
              success: bool,
              stability_score: u32,
              response_code: u16,
              total_tests: u8,
              passed_tests: u8,
              final_score: u64,
              response_size: usize,
              connection_quality: f64,
          }

          #[derive(Clone, Copy, Debug)]
          enum ScanMode {
              QuantumSpeed,
              UltraPrecise,
              HybridQuantumUltra,
          }

          fn main() {
              let args: Vec<String> = env::args().collect();
              if args.len() < 3 {
                  eprintln!("Usage: {} <input_file> <scan_mode>", args[0]);
                  std::process::exit(1);
              }
              
              let filename = &args[1];
              let scan_mode = match args[2].as_str() {
                  "quantum_speed" => ScanMode::QuantumSpeed,
                  "ultra_precise" => ScanMode::UltraPrecise,
                  _ => ScanMode::HybridQuantumUltra,
              };
              
              println!("ðŸš€ Quantum Scanner Engine v3.1 Initialized");
              println!("ðŸ“Š Mode: {:?}", scan_mode);
              
              let file = File::open(filename).expect("Cannot open input file");
              let reader = BufReader::new(file);
              let proxies: Vec<String> = reader.lines().filter_map(Result::ok).collect();
              
              println!("ðŸŽ¯ Loaded {} targets for evaluation", proxies.len());
              
              let results = Arc::new(Mutex::new(Vec::new()));
              let mut handles = vec![];
              
              let (chunk_size, threads) = match scan_mode {
                  ScanMode::QuantumSpeed => (8, 128),
                  ScanMode::UltraPrecise => (1, 32),
                  ScanMode::HybridQuantumUltra => (4, 64),
              };
              
              let total_chunks = (proxies.len() + chunk_size - 1) / chunk_size;
              println!("âš™ï¸  Spawning {} worker threads with chunk size {}", threads.min(total_chunks), chunk_size);
              
              for chunk in proxies.chunks(chunk_size) {
                  let chunk = chunk.to_vec();
                  let results = Arc::clone(&results);
                  
                  let handle = thread::spawn(move || {
                      for proxy in chunk {
                          let stats = evaluate_proxy_ultimate(&proxy, scan_mode);
                          if stats.success && stats.final_score > 0 {
                              let mut data = results.lock().unwrap();
                              data.push(stats);
                          }
                      }
                  });
                  
                  handles.push(handle);
                  
                  if handles.len() >= threads {
                      for h in handles.drain(..) {
                          let _ = h.join();
                      }
                  }
              }
              
              for handle in handles {
                  let _ = handle.join();
              }
              
              let mut data = results.lock().unwrap();
              println!("âœ… Scan complete: {} viable proxies discovered", data.len());
              
              data.sort_by(|a, b| b.final_score.cmp(&a.final_score));
              
              write_results(&data, "quantum_results.json");
              write_detailed_log(&data, "logs/detailed_results.txt");
              
              println!("ðŸ’¾ Results saved to quantum_results.json");
          }

          fn evaluate_proxy_ultimate(ip: &str, mode: ScanMode) -> ProxyNode {
              let addr_result = ip.parse::<SocketAddr>();
              if addr_result.is_err() {
                  return create_failed_node(ip);
              }
              let addr = addr_result.unwrap();
              
              let test_rounds: u8 = match mode {
                  ScanMode::QuantumSpeed => 1,
                  ScanMode::UltraPrecise => 3,
                  ScanMode::HybridQuantumUltra => 2,
              };
              
              let mut latencies = Vec::new();
              let mut ttfbs = Vec::new();
              let mut sizes = Vec::new();
              let mut passed = 0u8;
              let mut response_code = 0u16;
              
              for round in 0..test_rounds {
                  match perform_http_test(&addr, mode) {
                      Ok((lat, ttfb, code, size)) => {
                          latencies.push(lat);
                          ttfbs.push(ttfb);
                          sizes.push(size);
                          response_code = code;
                          passed += 1;
                      }
                      Err(_) => {
                          latencies.push(9999);
                          ttfbs.push(9999);
                          sizes.push(0);
                      }
                  }
                  
                  if mode as i32 >= ScanMode::UltraPrecise as i32 && round < test_rounds - 1 {
                      thread::sleep(Duration::from_millis(100));
                  }
              }
              
              let success = passed > 0;
              if !success {
                  return create_failed_node(ip);
              }
              
              let avg_latency = average(&latencies);
              let avg_ttfb = average(&ttfbs);
              let jitter = calculate_jitter(&latencies);
              let stability = calculate_stability(passed, test_rounds);
              let avg_size = average_usize(&sizes);
              let quality = calculate_connection_quality(avg_latency, avg_ttfb, jitter, stability);
              
              let final_score = calculate_ultimate_score_v3(
                  avg_latency,
                  avg_ttfb,
                  jitter,
                  stability,
                  response_code,
                  quality,
                  avg_size,
              );
              
              ProxyNode {
                  ip_port: ip.to_string(),
                  tcp_latency: avg_latency,
                  ttfb: avg_ttfb,
                  jitter,
                  success: true,
                  stability_score: stability,
                  response_code,
                  total_tests: test_rounds,
                  passed_tests: passed,
                  final_score,
                  response_size: avg_size,
                  connection_quality: quality,
              }
          }

          fn perform_http_test(addr: &SocketAddr, mode: ScanMode) -> Result<(u128, u128, u16, usize), String> {
              let timeout = match mode {
                  ScanMode::QuantumSpeed => Duration::from_secs(2),
                  ScanMode::UltraPrecise => Duration::from_secs(5),
                  ScanMode::HybridQuantumUltra => Duration::from_secs(3),
              };
              
              let start_tcp = Instant::now();
              let stream_result = TcpStream::connect_timeout(&addr, timeout);
              let tcp_time = start_tcp.elapsed().as_millis();
              
              if stream_result.is_err() {
                  return Err("TCP connection failed".to_string());
              }
              
              let mut stream = stream_result.unwrap();
              let _ = stream.set_read_timeout(Some(timeout));
              let _ = stream.set_write_timeout(Some(timeout));
              let _ = stream.set_nodelay(true);
              
              let request = "HEAD http://www.google.com/ HTTP/1.1\r\n\
                             Host: www.google.com\r\n\
                             User-Agent: Mozilla/5.0\r\n\
                             Proxy-Connection: close\r\n\
                             Connection: close\r\n\r\n";
              
              let start_ttfb = Instant::now();
              if stream.write_all(request.as_bytes()).is_err() {
                  return Err("Write failed".to_string());
              }
              
              let mut buffer = vec![0u8; 2048];
              let read_result = stream.read(&mut buffer);
              let ttfb_time = start_ttfb.elapsed().as_millis();
              
              match read_result {
                  Ok(n) if n > 0 => {
                      let response = String::from_utf8_lossy(&buffer[..n]);
                      let code = extract_http_code(&response);
                      
                      if response.contains("HTTP/") && is_valid_code(code) {
                          Ok((tcp_time, ttfb_time, code, n))
                      } else {
                          Err("Invalid HTTP response".to_string())
                      }
                  }
                  _ => Err("No data received".to_string()),
              }
          }

          fn calculate_ultimate_score_v3(
              latency: u128,
              ttfb: u128,
              jitter: u128,
              stability: u32,
              response_code: u16,
              quality: f64,
              response_size: usize,
          ) -> u64 {
              let total_time = latency + ttfb;
              if total_time == 0 {
                  return 0;
              }
              
              let mut score = (1_000_000 / total_time as u64).min(100_000);
              
              if latency < 30 {
                  score += 8000;
              } else if latency < 50 {
                  score += 6000;
              } else if latency < 80 {
                  score += 4000;
              } else if latency < 100 {
                  score += 2500;
              } else if latency < 150 {
                  score += 1200;
              } else if latency < 200 {
                  score += 500;
              }
              
              if ttfb < 50 {
                  score += 6000;
              } else if ttfb < 100 {
                  score += 4500;
              } else if ttfb < 150 {
                  score += 3000;
              } else if ttfb < 200 {
                  score += 1800;
              } else if ttfb < 300 {
                  score += 800;
              } else if ttfb < 400 {
                  score += 300;
              }
              
              score = (score * stability as u64) / 100;
              
              if jitter > 150 {
                  score = (score * 60) / 100;
              } else if jitter > 100 {
                  score = (score * 70) / 100;
              } else if jitter > 50 {
                  score = (score * 85) / 100;
              } else if jitter < 20 {
                  score += 1000;
              }
              
              match response_code {
                  200 => score += 1500,
                  301 | 302 | 307 | 308 => score += 800,
                  204 => score += 600,
                  _ => {}
              }
              
              score = ((score as f64) * quality) as u64;
              
              if response_size > 100 && response_size < 10000 {
                  score += 500;
              }
              
              score.min(999_999)
          }

          fn calculate_connection_quality(latency: u128, ttfb: u128, jitter: u128, stability: u32) -> f64 {
              let mut quality = 1.0_f64;
              
              if latency < 50 {
                  quality *= 1.15;
              } else if latency < 100 {
                  quality *= 1.08;
              } else if latency > 250 {
                  quality *= 0.9;
              }
              
              if ttfb < 100 {
                  quality *= 1.12;
              } else if ttfb > 400 {
                  quality *= 0.88;
              }
              
              if jitter < 20 {
                  quality *= 1.2;
              } else if jitter < 50 {
                  quality *= 1.05;
              } else if jitter > 100 {
                  quality *= 0.85;
              }
              
              if stability > 95 {
                  quality *= 1.15;
              } else if stability > 80 {
                  quality *= 1.05;
              } else if stability < 60 {
                  quality *= 0.9;
              }
              
              quality.min(1.5_f64).max(0.5_f64)
          }

          fn average(values: &[u128]) -> u128 {
              if values.is_empty() {
                  return 9999;
              }
              let valid: Vec<u128> = values.iter().filter(|&&x| x < 9000).copied().collect();
              if valid.is_empty() {
                  return 9999;
              }
              valid.iter().sum::<u128>() / valid.len() as u128
          }
          
          fn average_usize(values: &[usize]) -> usize {
              if values.is_empty() {
                  return 0;
              }
              let valid: Vec<usize> = values.iter().filter(|&&x| x > 0).copied().collect();
              if valid.is_empty() {
                  return 0;
              }
              valid.iter().sum::<usize>() / valid.len()
          }
          
          fn calculate_jitter(latencies: &[u128]) -> u128 {
              if latencies.len() < 2 {
                  return 0;
              }
              let valid: Vec<u128> = latencies.iter().filter(|&&x| x < 9000).copied().collect();
              if valid.len() < 2 {
                  return 0;
              }
              let avg = average(&valid);
              let variance: u128 = valid.iter().map(|&x| {
                  let diff = if x > avg { x - avg } else { avg - x };
                  diff * diff
              }).sum::<u128>() / valid.len() as u128;
              (variance as f64).sqrt() as u128
          }
          
          fn calculate_stability(passed: u8, total: u8) -> u32 {
              ((passed as f32 / total as f32) * 100.0) as u32
          }
          
          fn extract_http_code(response: &str) -> u16 {
              response.lines().next().and_then(|line| {
                  line.split_whitespace().nth(1).and_then(|code| code.parse().ok())
              }).unwrap_or(0)
          }
          
          fn is_valid_code(code: u16) -> bool {
              matches!(code, 200 | 201 | 204 | 301 | 302 | 304 | 307 | 308)
          }
          
          fn create_failed_node(ip: &str) -> ProxyNode {
              ProxyNode {
                  ip_port: ip.to_string(),
                  tcp_latency: 9999,
                  ttfb: 9999,
                  jitter: 9999,
                  success: false,
                  stability_score: 0,
                  response_code: 0,
                  total_tests: 0,
                  passed_tests: 0,
                  final_score: 0,
                  response_size: 0,
                  connection_quality: 0.0,
              }
          }
          
          fn write_results(data: &[ProxyNode], filename: &str) {
              let mut file = File::create(filename).expect("Cannot create output file");
              write!(file, "[").unwrap();
              
              for (i, node) in data.iter().enumerate() {
                  if i > 0 {
                      write!(file, ",").unwrap();
                  }
                  write!(
                      file,
                      "{{\"ip_port\":\"{}\",\"latency\":{},\"ttfb\":{},\"jitter\":{},\"stability\":{},\"response_code\":{},\"tests\":\"{}/{}\",\"score\":{},\"quality\":{:.3},\"size\":{}}}",
                      node.ip_port,
                      node.tcp_latency,
                      node.ttfb,
                      node.jitter,
                      node.stability_score,
                      node.response_code,
                      node.passed_tests,
                      node.total_tests,
                      node.final_score,
                      node.connection_quality,
                      node.response_size
                  ).unwrap();
              }
              
              write!(file, "]").unwrap();
          }
          
          fn write_detailed_log(data: &[ProxyNode], filename: &str) {
              let mut file = File::create(filename).expect("Cannot create log file");
              writeln!(file, "=== QUANTUM PROXY SCANNER v3.1 - DETAILED RESULTS ===\n").unwrap();
              writeln!(file, "Total Viable Proxies: {}\n", data.len()).unwrap();
              
              for (i, node) in data.iter().enumerate() {
                  writeln!(file, "Rank #{}: {}", i + 1, node.ip_port).unwrap();
                  writeln!(file, "  Score: {}", node.final_score).unwrap();
                  writeln!(file, "  Latency: {}ms", node.tcp_latency).unwrap();
                  writeln!(file, "  TTFB: {}ms", node.ttfb).unwrap();
                  writeln!(file, "  Jitter: {}ms", node.jitter).unwrap();
                  writeln!(file, "  Stability: {}%", node.stability_score).unwrap();
                  writeln!(file, "  HTTP Code: {}", node.response_code).unwrap();
                  writeln!(file, "  Tests Passed: {}/{}", node.passed_tests, node.total_tests).unwrap();
                  writeln!(file, "  Quality Index: {:.3}", node.connection_quality).unwrap();
                  writeln!(file, "  Response Size: {} bytes\n", node.response_size).unwrap();
              }
          }
          RUST_ENGINE_EOF
          
          echo "âœ… Advanced Rust engine v3.1 source generated"
          echo ""
          echo "ðŸ”¨ Compiling with maximum optimization flags..."
          
          rustc -C opt-level=3 \
                -C target-cpu=native \
                -C codegen-units=1 \
                -C lto=fat \
                -C panic=abort \
                -C strip=symbols \
                main.rs -o quantum_scanner
          
          if [ ! -f quantum_scanner ]; then
            echo "âŒ Compilation failed!"
            exit 1
          fi
          
          chmod +x quantum_scanner
          echo "âœ… Quantum Scanner v3.1 compiled successfully"
          
          ls -lh quantum_scanner
          echo ""

      - name: âš”ï¸ Execute Multi-Mode Quantum Scan
        run: |
          SCAN_MODE="${{ github.event.inputs.scan_mode || 'hybrid_quantum_ultra' }}"
          echo "ðŸŽ¯ Initiating $SCAN_MODE scanning protocol..."
          echo ""
          
          START_TIME=$(date +%s)
          
          ./quantum_scanner data/targets.txt "$SCAN_MODE" 2>&1 | tee logs/scanner_output.txt
          
          END_TIME=$(date +%s)
          DURATION=$((END_TIME - START_TIME))
          
          echo ""
          echo "âœ… Quantum scan completed in ${DURATION} seconds"
          
          if [ ! -s quantum_results.json ]; then
            echo "âš ï¸  No results generated, creating empty result set"
            echo "[]" > quantum_results.json
          fi
          
          RESULT_COUNT=$(jq '. | length' quantum_results.json 2>/dev/null || echo "0")
          echo "ðŸ“Š Generated $RESULT_COUNT viable proxy candidates"
          
          if [ "$RESULT_COUNT" -gt 0 ]; then
            echo ""
            echo "ðŸ† Top 3 Performers:"
            jq -r '.[:3] | .[] | "  â€¢ \(.ip_port) - Score: \(.score), Latency: \(.latency)ms"' quantum_results.json 2>/dev/null || true
          fi

      - name: ðŸ¤– Advanced Neural Selection Engine v3.1 - Top 3 Elite
        id: neural_selection
        env:
          MIN_SCORE: ${{ github.event.inputs.min_score_threshold || '500' }}
        run: |
          echo "ðŸ§® Activating enhanced multi-dimensional neural analysis..."
          echo "ðŸŽ¯ Elite Selection Mode: TOP 3 PERFORMERS ONLY"
          echo ""
          
          RESULT_COUNT=$(jq '. | length' quantum_results.json 2>/dev/null || echo "0")
          
          if [ "$RESULT_COUNT" -eq 0 ]; then
            echo "âš ï¸  No viable proxies found, using failover configuration"
            BEST_IP="127.0.0.1:8080"
            BEST_SCORE=0
            BEST_LAT=0
            BEST_TTFB=0
            BEST_JITTER=0
            BEST_STABILITY=0
            BEST_QUALITY=0.0
            BEST_LOC="XX"
          else
            echo "ðŸŽ¯ Analyzing $RESULT_COUNT candidates with advanced AI scoring..."
            
            FILTERED=$(jq --arg min "$MIN_SCORE" '[.[] | select(.score >= ($min | tonumber) and .quality >= 0.8)]' quantum_results.json 2>/dev/null || echo "[]")
            FILTERED_COUNT=$(echo "$FILTERED" | jq '. | length' 2>/dev/null || echo "0")
            
            echo "âœ… $FILTERED_COUNT proxies meet quality standards (score â‰¥ $MIN_SCORE, quality â‰¥ 0.8)"
            
            if [ "$FILTERED_COUNT" -eq 0 ]; then
              echo "â„¹ï¸  Relaxing quality threshold..."
              FILTERED=$(jq --arg min "$MIN_SCORE" '[.[] | select(.score >= ($min | tonumber))]' quantum_results.json 2>/dev/null || echo "[]")
              FILTERED_COUNT=$(echo "$FILTERED" | jq '. | length' 2>/dev/null || echo "0")
              echo "âœ… $FILTERED_COUNT proxies meet minimum score threshold"
            fi
            
            if [ "$FILTERED_COUNT" -eq 0 ]; then
              echo "âš ï¸  No proxies meet threshold, selecting best available"
              FILTERED=$(jq '.' quantum_results.json 2>/dev/null || echo "[]")
            fi
            
            echo ""
            echo "ðŸ† === SELECTING TOP 3 ELITE PERFORMERS ==="
            echo ""
            
            echo "$FILTERED" | jq -c 'sort_by(-.score) | .[:3]' > results/top3_elite.json 2>/dev/null || echo "[]" > results/top3_elite.json
            
            TOP3_COUNT=$(jq '. | length' results/top3_elite.json 2>/dev/null || echo "0")
            echo "âœ… Top $TOP3_COUNT elite performers selected for database storage"
            
            BEST_NODE=$(jq -c '.[0]' results/top3_elite.json 2>/dev/null || echo "{}")
            
            BEST_IP=$(echo "$BEST_NODE" | jq -r '.ip_port // "127.0.0.1:8080"' 2>/dev/null)
            BEST_SCORE=$(echo "$BEST_NODE" | jq -r '.score // 0' 2>/dev/null)
            BEST_LAT=$(echo "$BEST_NODE" | jq -r '.latency // 0' 2>/dev/null)
            BEST_TTFB=$(echo "$BEST_NODE" | jq -r '.ttfb // 0' 2>/dev/null)
            BEST_JITTER=$(echo "$BEST_NODE" | jq -r '.jitter // 0' 2>/dev/null)
            BEST_STABILITY=$(echo "$BEST_NODE" | jq -r '.stability // 0' 2>/dev/null)
            BEST_QUALITY=$(echo "$BEST_NODE" | jq -r '.quality // 0.0' 2>/dev/null)
            
            CLEAN_IP=$(echo "$BEST_IP" | cut -d':' -f1)
            BEST_LOC=$(geoiplookup "$CLEAN_IP" 2>/dev/null | awk -F': ' '{print $2}' | cut -d',' -f1 | head -c 2 || echo "XX")
            if [ -z "$BEST_LOC" ] || [ "$BEST_LOC" == "IP" ]; then
              BEST_LOC="XX"
            fi
            
            echo ""
            echo "ðŸ† =============================================="
            echo "ðŸ† TOP 3 ELITE PERFORMERS"
            echo "ðŸ† =============================================="
            jq -r '.[] | "  #\(. | tostring | split(",") | length). \(.ip_port) - Score: \(.score), Latency: \(.latency)ms, Quality: \(.quality)"' results/top3_elite.json 2>/dev/null | head -3 | nl
            echo "ðŸ† =============================================="
            echo ""
            echo "ðŸ‘‘ CHAMPION (Rank #1): $BEST_IP"
            echo "   Neural Score: $BEST_SCORE"
            echo "   Latency: ${BEST_LAT}ms"
            echo "   TTFB: ${BEST_TTFB}ms"
            echo "   Jitter: ${BEST_JITTER}ms"
            echo "   Stability: ${BEST_STABILITY}%"
            echo "   Quality Index: $BEST_QUALITY"
            echo "   Location: $BEST_LOC"
            echo "ðŸ† =============================================="
            echo ""
          fi
          
          echo "best_ip=$BEST_IP" >> $GITHUB_OUTPUT
          echo "best_score=$BEST_SCORE" >> $GITHUB_OUTPUT
          echo "best_lat=$BEST_LAT" >> $GITHUB_OUTPUT
          echo "best_ttfb=$BEST_TTFB" >> $GITHUB_OUTPUT
          echo "best_jitter=$BEST_JITTER" >> $GITHUB_OUTPUT
          echo "best_stability=$BEST_STABILITY" >> $GITHUB_OUTPUT
          echo "best_quality=$BEST_QUALITY" >> $GITHUB_OUTPUT
          echo "best_loc=$BEST_LOC" >> $GITHUB_OUTPUT
          echo "result_count=$RESULT_COUNT" >> $GITHUB_OUTPUT
          echo "top3_count=$TOP3_COUNT" >> $GITHUB_OUTPUT

      - name: ðŸ’¾ Cloudflare D1 - Atomic Top 3 Elite Storage System
        id: db_sync
        continue-on-error: true
        env:
          CF_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CF_ACCOUNT_ID: ${{ secrets.CF_ACCOUNT_ID }}
          D1_DATABASE_ID: ${{ secrets.D1_DATABASE_ID }}
        run: |
          if [ -z "$CF_API_TOKEN" ]; then
            echo "â„¹ï¸  D1 database not configured, skipping sync"
            exit 0
          fi
          
          echo "ðŸ’¾ === ATOMIC TOP 3 ELITE STORAGE SYSTEM ==="
          echo "ðŸŽ¯ Only the best 3 proxies will remain in database"
          echo ""
          
          TIMESTAMP=$(date +%s)
          
          SQL_SCHEMA="CREATE TABLE IF NOT EXISTS proxy_health (
            ip_port TEXT PRIMARY KEY,
            total_score INTEGER,
            latency_ms INTEGER,
            ttfb_ms INTEGER,
            jitter_ms INTEGER,
            stability_pct INTEGER,
            quality_index REAL DEFAULT 1.0,
            location TEXT,
            last_check INTEGER,
            check_count INTEGER DEFAULT 1,
            success_rate REAL DEFAULT 100.0,
            is_healthy INTEGER,
            is_current_best INTEGER DEFAULT 0,
            first_seen INTEGER,
            best_score INTEGER,
            last_champion_time INTEGER DEFAULT 0,
            consecutive_successes INTEGER DEFAULT 1,
            consecutive_failures INTEGER DEFAULT 0
          );"
          
          curl -s --max-time 10 -X POST \
            "https://api.cloudflare.com/client/v4/accounts/$CF_ACCOUNT_ID/d1/database/$D1_DATABASE_ID/query" \
            -H "Authorization: Bearer $CF_API_TOKEN" \
            -H "Content-Type: application/json" \
            --data "$(jq -n --arg sql "$SQL_SCHEMA" '{sql: $sql}')" > /dev/null 2>&1
          
          echo "âœ… Enhanced schema validated"
          
          echo "ðŸ—‘ï¸  Phase 1: Removing ALL existing proxies..."
          
          SQL_DELETE_ALL="DELETE FROM proxy_health;"
          
          curl -s --max-time 15 -X POST \
            "https://api.cloudflare.com/client/v4/accounts/$CF_ACCOUNT_ID/d1/database/$D1_DATABASE_ID/query" \
            -H "Authorization: Bearer $CF_API_TOKEN" \
            -H "Content-Type: application/json" \
            --data "$(jq -n --arg sql "$SQL_DELETE_ALL" '{sql: $sql}')" > /dev/null 2>&1
          
          echo "âœ… Database cleared - fresh start"
          
          echo "ðŸ’Ž Phase 2: Inserting TOP 3 ELITE performers..."
          
          if [ ! -f results/top3_elite.json ]; then
            echo "âš ï¸  Top 3 file not found, skipping insert"
            exit 0
          fi
          
          TOP3_COUNT=$(jq '. | length' results/top3_elite.json 2>/dev/null || echo "0")
          
          if [ "$TOP3_COUNT" -eq 0 ]; then
            echo "âš ï¸  No elite proxies to insert"
            exit 0
          fi
          
          echo "ðŸ“Š Processing $TOP3_COUNT elite proxies for insertion..."
          
          RANK=1
          jq -c '.[]' results/top3_elite.json 2>/dev/null | while read -r proxy; do
            P_IP=$(echo "$proxy" | jq -r '.ip_port // ""' 2>/dev/null)
            P_SCORE=$(echo "$proxy" | jq -r '.score // 0' 2>/dev/null)
            P_LAT=$(echo "$proxy" | jq -r '.latency // 0' 2>/dev/null)
            P_TTFB=$(echo "$proxy" | jq -r '.ttfb // 0' 2>/dev/null)
            P_JITTER=$(echo "$proxy" | jq -r '.jitter // 0' 2>/dev/null)
            P_STAB=$(echo "$proxy" | jq -r '.stability // 0' 2>/dev/null)
            P_QUAL=$(echo "$proxy" | jq -r '.quality // 1.0' 2>/dev/null)
            
            if [ -z "$P_IP" ]; then
              continue
            fi
            
            P_CLEAN_IP=$(echo "$P_IP" | cut -d':' -f1)
            P_LOC=$(geoiplookup "$P_CLEAN_IP" 2>/dev/null | awk -F': ' '{print $2}' | cut -d',' -f1 | head -c 2 || echo "XX")
            
            IS_CHAMPION=0
            if [ "$RANK" -eq 1 ]; then
              IS_CHAMPION=1
              echo "ðŸ‘‘ Inserting CHAMPION: $P_IP (Score: $P_SCORE)"
            else
              echo "ðŸ’Ž Inserting Elite #$RANK: $P_IP (Score: $P_SCORE)"
            fi
            
            SQL_INSERT="INSERT INTO proxy_health (
              ip_port, total_score, latency_ms, ttfb_ms, jitter_ms, 
              stability_pct, quality_index, location, last_check, is_healthy, 
              is_current_best, first_seen, best_score, check_count, 
              last_champion_time, consecutive_successes, success_rate
            ) VALUES (
              '$P_IP', $P_SCORE, $P_LAT, $P_TTFB, $P_JITTER,
              $P_STAB, $P_QUAL, '$P_LOC', $TIMESTAMP, 1,
              $IS_CHAMPION, $TIMESTAMP, $P_SCORE, 1,
              $TIMESTAMP, 1, 100.0
            );"
            
            INSERT_RESP=$(curl -s --max-time 10 -X POST \
              "https://api.cloudflare.com/client/v4/accounts/$CF_ACCOUNT_ID/d1/database/$D1_DATABASE_ID/query" \
              -H "Authorization: Bearer $CF_API_TOKEN" \
              -H "Content-Type: application/json" \
              --data "$(jq -n --arg sql "$SQL_INSERT" '{sql: $sql}')" 2>/dev/null)
            
            if echo "$INSERT_RESP" | grep -q '"success":true' 2>/dev/null; then
              if [ "$IS_CHAMPION" -eq 1 ]; then
                echo "  âœ… Champion inserted successfully"
              else
                echo "  âœ… Elite proxy inserted"
              fi
            else
              echo "  âš ï¸  Insert warning for $P_IP"
            fi
            
            RANK=$((RANK + 1))
          done
          
          echo ""
          echo "ðŸ” Phase 3: Verification..."
          
          SQL_VERIFY="SELECT COUNT(*) as total, 
                             SUM(CASE WHEN is_current_best = 1 THEN 1 ELSE 0 END) as champions,
                             MIN(total_score) as min_score,
                             MAX(total_score) as max_score
                      FROM proxy_health;"
          
          VERIFY_RESP=$(curl -s --max-time 10 -X POST \
            "https://api.cloudflare.com/client/v4/accounts/$CF_ACCOUNT_ID/d1/database/$D1_DATABASE_ID/query" \
            -H "Authorization: Bearer $CF_API_TOKEN" \
            -H "Content-Type: application/json" \
            --data "$(jq -n --arg sql "$SQL_VERIFY" '{sql: $sql}')" 2>/dev/null)
          
          DB_TOTAL=$(echo "$VERIFY_RESP" | jq -r '.result[0].results[0].total // 0' 2>/dev/null)
          DB_CHAMPIONS=$(echo "$VERIFY_RESP" | jq -r '.result[0].results[0].champions // 0' 2>/dev/null)
          DB_MIN_SCORE=$(echo "$VERIFY_RESP" | jq -r '.result[0].results[0].min_score // 0' 2>/dev/null)
          DB_MAX_SCORE=$(echo "$VERIFY_RESP" | jq -r '.result[0].results[0].max_score // 0' 2>/dev/null)
          
          echo ""
          echo "ðŸ“Š === DATABASE VERIFICATION ==="
          echo "  â€¢ Total Proxies in DB: $DB_TOTAL"
          echo "  â€¢ Champions (should be 1): $DB_CHAMPIONS"
          echo "  â€¢ Score Range: $DB_MIN_SCORE - $DB_MAX_SCORE"
          echo ""
          
          if [ "$DB_TOTAL" -le 3 ] && [ "$DB_CHAMPIONS" -eq 1 ]; then
            echo "âœ… âœ… âœ… PERFECT! Top 3 system working correctly!"
          else
            echo "âš ï¸  Verification warning - expected â‰¤3 total, 1 champion"
          fi
          
          echo ""
          echo "champion_verified=$DB_CHAMPIONS" >> $GITHUB_OUTPUT
          echo "total_in_db=$DB_TOTAL" >> $GITHUB_OUTPUT

      - name: ðŸ” Enhanced Verification & Quality Assurance
        id: verification
        run: |
          echo "ðŸ” Performing comprehensive quality assurance checks..."
          
          BEST_IP="${{ steps.neural_selection.outputs.best_ip }}"
          BEST_SCORE="${{ steps.neural_selection.outputs.best_score }}"
          BEST_QUALITY="${{ steps.neural_selection.outputs.best_quality }}"
          
          PROXY_IP=$(echo "$BEST_IP" | cut -d':' -f1)
          PROXY_PORT=$(echo "$BEST_IP" | cut -d':' -f2)
          
          echo "ðŸŽ¯ Verifying champion proxy: $BEST_IP"
          echo ""
          
          echo "ðŸ“¡ Testing port connectivity..."
          if timeout 5 nc -zv "$PROXY_IP" "$PROXY_PORT" 2>&1 | grep -q succeeded; then
            echo "âœ… Port $PROXY_PORT is reachable"
            echo "port_check=pass" >> $GITHUB_OUTPUT
          else
            echo "âš ï¸  Port check inconclusive"
            echo "port_check=warn" >> $GITHUB_OUTPUT
          fi
          
          echo "ðŸ” Validating IP address format..."
          if [[ $PROXY_IP =~ ^[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}$ ]]; then
            echo "âœ… IP format valid: $PROXY_IP"
            echo "ip_check=pass" >> $GITHUB_OUTPUT
          else
            echo "âš ï¸  IP format warning"
            echo "ip_check=warn" >> $GITHUB_OUTPUT
          fi
          
          LAT="${{ steps.neural_selection.outputs.best_lat }}"
          
          COMPOSITE_SCORE=0
          
          if [ "$BEST_SCORE" -gt 10000 ]; then
            GRADE="S+ (Elite Champion)"
            COMPOSITE_SCORE=$((COMPOSITE_SCORE + 50))
          elif [ "$BEST_SCORE" -gt 7000 ]; then
            GRADE="S (Exceptional)"
            COMPOSITE_SCORE=$((COMPOSITE_SCORE + 45))
          elif [ "$BEST_SCORE" -gt 5000 ]; then
            GRADE="A+ (Excellent)"
            COMPOSITE_SCORE=$((COMPOSITE_SCORE + 40))
          elif [ "$BEST_SCORE" -gt 3000 ]; then
            GRADE="A (Very Good)"
            COMPOSITE_SCORE=$((COMPOSITE_SCORE + 35))
          elif [ "$BEST_SCORE" -gt 1500 ]; then
            GRADE="B (Good)"
            COMPOSITE_SCORE=$((COMPOSITE_SCORE + 25))
          else
            GRADE="C (Average)"
            COMPOSITE_SCORE=$((COMPOSITE_SCORE + 15))
          fi
          
          QUALITY_INT=$(echo "$BEST_QUALITY * 100" | bc 2>/dev/null || echo "100")
          if [ "$QUALITY_INT" -gt 120 ]; then
            COMPOSITE_SCORE=$((COMPOSITE_SCORE + 30))
          elif [ "$QUALITY_INT" -gt 100 ]; then
            COMPOSITE_SCORE=$((COMPOSITE_SCORE + 20))
          elif [ "$QUALITY_INT" -gt 90 ]; then
            COMPOSITE_SCORE=$((COMPOSITE_SCORE + 10))
          fi
          
          if [ "$LAT" -lt 50 ]; then
            COMPOSITE_SCORE=$((COMPOSITE_SCORE + 20))
          elif [ "$LAT" -lt 100 ]; then
            COMPOSITE_SCORE=$((COMPOSITE_SCORE + 10))
          fi
          
          echo "performance_grade=$GRADE" >> $GITHUB_OUTPUT
          echo "composite_score=$COMPOSITE_SCORE" >> $GITHUB_OUTPUT
          
          echo ""
          echo "ðŸ“Š Performance Assessment:"
          echo "  â€¢ Grade: $GRADE"
          echo "  â€¢ Composite Score: $COMPOSITE_SCORE/100"
          echo ""

      - name: ðŸ“Š Generate Ultimate Quantum Report v3.1 - Top 3 Edition
        if: always()
        run: |
          echo "ðŸ“ Generating comprehensive analysis report..."
          
          cat > results/ultimate_report.md << 'REPORT_EOF'
          # ðŸŒŒ Ultimate Quantum-AI Proxy Evolution Report v3.1
          ## Top 3 Elite Edition
          
          ## ðŸ† Champion Proxy Selection [TOP 3 SYSTEM]
          
          **Selected Champion Node:** `${{ steps.neural_selection.outputs.best_ip }}`
          
          **Database Status:** ${{ steps.db_sync.outputs.total_in_db || 'N/A' }} proxies stored (Maximum: 3)
          
          **Champion Verification:** ${{ steps.db_sync.outputs.champion_verified == '1' && 'âœ… Single Champion Verified' || 'âš ï¸ Verification Pending' }}
          
          ### ðŸ“Š Enhanced Performance Metrics
          
          | Metric | Value | Status |
          |:-------|:------|:-------|
          | **Neural Score** | `${{ steps.neural_selection.outputs.best_score }}` | ${{ steps.verification.outputs.performance_grade }} |
          | **TCP Latency** | `${{ steps.neural_selection.outputs.best_lat }} ms` | âš¡ Speed |
          | **TTFB** | `${{ steps.neural_selection.outputs.best_ttfb }} ms` | ðŸš€ Response |
          | **Jitter** | `${{ steps.neural_selection.outputs.best_jitter }} ms` | ðŸ“Š Variance |
          | **Stability** | `${{ steps.neural_selection.outputs.best_stability }}%` | âœ… Reliability |
          | **Quality Index** | `${{ steps.neural_selection.outputs.best_quality }}` | ðŸ’Ž Excellence |
          | **Location** | `${{ steps.neural_selection.outputs.best_loc }}` | ðŸŒ Region |
          | **Composite Score** | `${{ steps.verification.outputs.composite_score }}/100` | ðŸŽ¯ Overall |
          
          ### ðŸŽ¯ Scan Statistics
          
          The system evaluated a comprehensive dataset through advanced multi-dimensional analysis using the `${{ github.event.inputs.scan_mode || 'hybrid_quantum_ultra' }}` mode. Successfully discovered `${{ steps.neural_selection.outputs.result_count }}` viable candidates. The AI memory system recalled `${{ steps.memory_recall.outputs.memory_count || '0' }}` high-performance nodes from historical data.
          
          ### ðŸ’Ž Top 3 Elite System
          
          This workflow implements an aggressive elite-only storage system. After each scan, the database is completely cleared and only the top three highest-performing proxies are stored. This ensures the database always contains only the absolute best performers from the most recent scan.
          
          The top three proxies are selected based on a comprehensive neural scoring algorithm that evaluates TCP latency, time to first byte, network jitter consistency, connection stability across multiple tests, quality index calculations, and response validation metrics. Each proxy undergoes rigorous testing before being considered for elite status.
          
          ### ðŸ§¹ Database Maintenance
          
          ${{ steps.db_cleanup.outputs.cleanup_performed == 'true' && format('Intelligent cleanup removed `{0}` outdated proxies. The database now maintains exactly `{1}` elite performers.', steps.db_cleanup.outputs.removed_count, steps.db_cleanup.outputs.remaining_count) || 'Database cleanup was skipped (D1 not configured).' }}
          
          ### ðŸ§  Atomic Champion Selection System
          
          The champion selection uses a two-phase atomic operation. First, the entire database is cleared to ensure a fresh start. Second, the top three proxies from the current scan are inserted, with the highest scorer receiving the champion flag with is_current_best equals one.
          
          This approach guarantees exactly three proxies in the database at any time, with exactly one champion. Applications can retrieve the current best performer with a simple query filtering for is_current_best equals one, instantly accessing the optimal proxy.
          
          ### ðŸ” Verification Results
          
          Port connectivity testing returned `${{ steps.verification.outputs.port_check || 'N/A' }}`. IP validation confirmed `${{ steps.verification.outputs.ip_check || 'N/A' }}`. Performance grading assigned `${{ steps.verification.outputs.performance_grade || 'N/A' }}` based on comprehensive metrics. Database verification shows `${{ steps.db_sync.outputs.champion_verified || 'N/A' }}` active champion with `${{ steps.db_sync.outputs.total_in_db || 'N/A' }}` total proxies stored.
          
          ### ðŸŽ¯ Query Current Champion
          
          To retrieve the current best proxy from your D1 database:
          
          ```sql
          SELECT ip_port, total_score, latency_ms, ttfb_ms, 
                 stability_pct, quality_index, location, 
                 last_champion_time, consecutive_successes
          FROM proxy_health 
          WHERE is_current_best = 1;
          ```
          
          This query returns exactly one row with the current champion and all performance metrics.
          
          ### ðŸ“‹ Query All Top 3
          
          To retrieve all three elite proxies:
          
          ```sql
          SELECT ip_port, total_score, latency_ms, ttfb_ms,
                 stability_pct, quality_index, location,
                 is_current_best
          FROM proxy_health
          ORDER BY total_score DESC;
          ```
          
          This query returns all three elite performers ranked by score, with the champion clearly marked.
          
          ---
          
          *Generated by Ultimate Quantum-AI Neural Engine v3.1*  
          *Top 3 Elite Edition - Maximum Performance*  
          *Timestamp: $(date -u +"%Y-%m-%d %H:%M:%S UTC")*  
          *Run: #${{ github.run_number }}*
          REPORT_EOF
          
          eval "echo \"$(cat results/ultimate_report.md)\"" > results/ultimate_report_final.md 2>/dev/null || cp results/ultimate_report.md results/ultimate_report_final.md
          
          cat results/ultimate_report_final.md >> $GITHUB_STEP_SUMMARY 2>/dev/null || true
          
          echo "âœ… Comprehensive report generated"

      - name: ðŸ“¦ Archive Results & Artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: quantum-proxy-top3-elite-run-${{ github.run_number }}
          path: |
            quantum_results.json
            results/
            logs/
            backups/
          retention-days: 30
          if-no-files-found: warn

      - name: ðŸŽ‰ Mission Complete
        if: success()
        run: |
          echo ""
          echo "ðŸŽ‰ =============================================="
          echo "ðŸŽ‰ QUANTUM EVOLUTION CYCLE COMPLETE v3.1"
          echo "ðŸŽ‰ TOP 3 ELITE EDITION"
          echo "ðŸŽ‰ =============================================="
          echo ""
          echo "âœ… All systems nominal"
          echo "âœ… Champion proxy selected and verified"
          echo "âœ… Database contains ONLY top 3 performers"
          echo "âœ… Enhanced quality metrics recorded"
          echo ""
          echo "ðŸ“‹ Quick Access:"
          echo "   â€¢ Champion: ${{ steps.neural_selection.outputs.best_ip }}"
          echo "   â€¢ Neural Score: ${{ steps.neural_selection.outputs.best_score }}"
          echo "   â€¢ Grade: ${{ steps.verification.outputs.performance_grade }}"
          echo "   â€¢ DB Total: ${{ steps.db_sync.outputs.total_in_db || '0' }} (Max: 3)"
          echo ""
          echo "ðŸ’Ž Top 3 Elite System Status:"
          echo "   â€¢ Database Cleaned: âœ…"
          echo "   â€¢ Only Best 3 Stored: âœ…"
          echo "   â€¢ Champion Flag Set: âœ…"
          echo ""

  cleanup-old-runs:
    name: ðŸ§¹ Cleanup Old Workflow Runs
    runs-on: ubuntu-latest
    needs: quantum-neural-ultimate-evolution
    if: always()
    permissions:
      actions: write
      contents: read
    
    steps:
      - name: ðŸ—‘ï¸ Delete Old Workflow Runs
        uses: Mattraks/delete-workflow-runs@v2
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          repository: ${{ github.repository }}
          retain_days: 0
          keep_minimum_runs: 0
